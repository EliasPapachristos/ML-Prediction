{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Model 'hits'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.externals import joblib\n",
    "from collections import Counter\n",
    "from bisect import bisect\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "import webbrowser\n",
    "import os\n",
    "\n",
    "# Set charts to view inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ML_Data_Scientist_Case_Study_Data.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the \"\\N\" from the \"hits\" column to NaN\n",
    "df = df.replace({r'\\N': np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry_Page, Session_Duration, Agent_id, Day_of_Week, Locale\n",
    "df.entry_page = df.entry_page.astype(str)\n",
    "# Spelling problem\n",
    "df = df.rename(columns={'session_durantion': 'session_duration'})\n",
    "df = pd.concat([df, pd.get_dummies(df.agent_id, prefix='agent')], axis=1)\n",
    "df['day_of_week'] = df['day_of_week'].map({\n",
    "   'Monday': 1, \n",
    "   'Tuesday': 2, \n",
    "   'Wednesday': 3, \n",
    "   'Thursday': 4, \n",
    "   'Friday': 5,\n",
    "   'Saturday': 6, \n",
    "   'Sunday': 7\n",
    "})\n",
    "df['locale'] = df['locale'].map({\n",
    "    'L1': 1,\n",
    "    'L2': 2,\n",
    "    'L3': 3,\n",
    "    'L4': 4,\n",
    "    'L5': 5,\n",
    "    'L6': 6\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To_Numeric\n",
    "df.session_duration = pd.to_numeric(df.session_duration)\n",
    "df.hits = pd.to_numeric(df.hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.loc[~df['hits'].isnull()]\n",
    "df_test = df.loc[df['hits'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check the path_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.assign(len_path = lambda r: r.path_id_set.str.split(';').str.len())\n",
    "df_train.len_path = df_train.len_path.fillna(1)\n",
    "df_test = df_train.assign(len_path = lambda r: r.path_id_set.str.split(';').str.len())\n",
    "df_test.len_path = df_test.len_path.fillna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_avengers = df.path_id_set.str.cat(sep=';')\n",
    "path_id_size = len(set(all_avengers.split(';')))\n",
    "path_id_freq = Counter(all_avengers.split(';'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['path_id_set'] = df.path_id_set.str.replace(';', '').astype(float)\n",
    "df_train.path_id_set = df_train.path_id_set.fillna('0')\n",
    "df_test.path_id_set = df_test.path_id_set.fillna('0')\n",
    "\n",
    "new_columns = ['path{}'.format(i) for i in range(8)]\n",
    "thresh = [2,4,15,41,500,13000,100000]\n",
    "values = [new_columns[bisect(thresh, v)] for v in path_id_freq.values()]\n",
    "page_to_category = dict(zip(path_id_freq.keys(), values))\n",
    "\n",
    "def infinity_war(x, col_name, dic):\n",
    "    paths = x.split(';')\n",
    "    ans = dict(zip(col_name, [0] * len(col_name)))    \n",
    "    for f in paths:        \n",
    "        ans[ dic[f] ] = 1\n",
    "    return pd.Series(ans)\n",
    "\n",
    "df_train = df_train.merge(df_train.path_id_set.apply(infinity_war, args=(new_columns, page_to_category)),\n",
    "                  left_index=True, right_index=True)\n",
    "df_test = df_test.merge(df_test.path_id_set.apply(infinity_war, args=(new_columns, page_to_category)),\n",
    "                  left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session_Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the duration that a user has at the (mainly) Landing Page\n",
    "median_session_duration_train  = df_train.groupby(['day_of_week', 'hour_of_day']).median()\n",
    "df_train.session_duration = df_train.apply(lambda elp: median_session_duration_train.loc[elp.day_of_week, elp.hour_of_day]['session_duration'] \n",
    "                                           if np.isnan(elp.session_duration) else elp.session_duration, axis=1)\n",
    "median_session_duration_test = df_test.groupby(['day_of_week', 'hour_of_day']).median()\n",
    "df_test.session_duration = df_test.apply(lambda elp: median_session_duration_test.loc[elp.day_of_week, elp.hour_of_day]['session_duration'] \n",
    "                                           if np.isnan(elp.session_duration) else elp.session_duration, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's time for the Entry_Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9936784463340552"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry_page_count = df.entry_page.value_counts()\n",
    "sum(entry_page_count.head(20)) * 1./sum(entry_page_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3332\n"
     ]
    }
   ],
   "source": [
    "# I'm printing the Top-10\n",
    "print(entry_page_count.head(10)[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the number we got from the Top-10\n",
    "df_train.entry_page = df_train.apply(lambda elp: 'misc' if entry_page_count.loc[elp.entry_page] < 3332\n",
    " else elp.entry_page, axis=1)\n",
    "df_test.entry_page = df_test.apply(lambda elp: 'misc' if entry_page_count.loc[elp.entry_page] < 3332\n",
    " else elp.entry_page, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features_df\n",
    "def get_ready_df(df_frame):\n",
    "    features_df = df_frame.copy()\n",
    "    features_df = pd.concat([features_df, pd.get_dummies(features_df.traffic_type)], axis=1)\n",
    "    features_df = pd.concat([features_df, pd.get_dummies(features_df.entry_page)], axis=1)\n",
    "    features_df = pd.concat([features_df, pd.get_dummies(features_df.hour_of_day)], axis=1)\n",
    "    features_df = pd.concat([features_df, pd.get_dummies(features_df.day_of_week)], axis=1)\n",
    "    features_df = features_df.drop(['row_num', 'day_of_week', 'locale', 'traffic_type', 'hour_of_day', 'entry_page', 'path_id_set'], axis=1)\n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data set in a training set (70%) and a test set (30%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wolverin = get_ready_df(df_train)\n",
    "X_train, X_test, y_train, y_test = train_test_split(wolverin.drop('hits', axis=1), \n",
    "                                                    wolverin.hits, test_size=0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostRegressor(base_estimator=DecisionTreeRegressor(criterion='mse', max_depth=10, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best'),\n",
       "         learning_rate=0.5, loss='square', n_estimators=20,\n",
       "         random_state=<mtrand.RandomState object at 0x00000236C432FF30>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deadpool_2 = np.random.RandomState(69)\n",
    "model = AdaBoostRegressor(\n",
    "    DecisionTreeRegressor(\n",
    "        max_depth=10), \n",
    "    loss ='square', \n",
    "    random_state = deadpool_2, \n",
    "    n_estimators = 20, \n",
    "    learning_rate = 0.5\n",
    ")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Model (alternative)\n",
    "If you have time you can stop the above Model\n",
    "and try the next 5 steps.\n",
    "But be prepared for a      long     time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ensemble.GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters we want to try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#    'n_estimators': [500, 1000, 3000],\n",
    "#    'max_depth': [4, 6],\n",
    "#    'min_samples_leaf': [3, 5, 9, 17],\n",
    "#    'learning_rate': [0.1, 0.05, 0.02, 0.01],\n",
    "#    'max_features': [1.0, 0.3, 0.1],\n",
    "#    'loss': ['ls', 'lad', 'huber']\n",
    "#}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the grid search we want to run. Run it with four cpus in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs_cv = GridSearchCV(model, param_grid, n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the grid search - on only the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the parameters that gave us the best result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gs_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model to a file so we can use it in other programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trained_hits_model.pkl']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to use it remote\n",
    "joblib.dump(model, 'trained_hits_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean_Squared_Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.816781657520359"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results/Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = get_ready_df(df_test)\n",
    "# if you are working with the deadpool_2 model DON'T change anything\n",
    "# If not, remove the '# 2' and put hashtag on the next line\n",
    "df_test['hits'] = model.predict(df_test.drop(columns='hits'))\n",
    "# 2 df_test['hits'] = gs_cv.predict(df_test.drop(columns='hits'))\n",
    "df_test['hits'] = df_test['hits'].round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['row_num'] = df.loc[df['hits'].isnull()].row_num\n",
    "final_csv = pd.concat([df.loc[df['hits'].isnull()].row_num, df_test['hits']], axis=1)\n",
    "final_csv.to_csv('submit_and_party.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the Data in a web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Web Page to view the Data easy\n",
    "html = df[0:100].to_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the HTML to a temporary file\n",
    "with open('data_new.html', 'w') as elp:\n",
    "    elp.write(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On our Web Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the Web Page with our favorite Browser\n",
    "full_file_name = os.path.abspath('data_new.html')\n",
    "webbrowser.open('file://{}'.format(full_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
